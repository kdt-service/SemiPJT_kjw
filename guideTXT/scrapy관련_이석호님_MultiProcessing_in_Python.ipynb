{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trWWdrnwz2Re"
   },
   "source": [
    "# MultiProcessing in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-Hw0KcbIduMt"
   },
   "source": [
    "## 멀티 프로세싱\n",
    "\n",
    "### 멀티프로세스란?\n",
    "프로세스(Processes란 **실행 중인 프로그램**을 의미 즉, 멀티 프로세스란 이 프로세스를 여러개 사용한다는 의미. 병렬처리 개념으로 이해하면 될 것 같습니다.</br>\n",
    "간단한 예시로 스테이크와 스테이크 소스 두 가지를 만들어야 하는 상황일때 사용할 수 있는 가스레인지 모델이 1구인 모델과 2구인 모델의 음식을 완성까지의 시간은 다르겠죠?</br></br>\n",
    "\n",
    "Python에서는 **multiprocessing**이라는 모듈이 내장되어 있어 이를 쉽게 구현할 수 있다고 합니다.\n",
    "\n",
    "</br>\n",
    "\n",
    "크롤링에 적용하기 이전에 간단한 예제로 멀티프로세싱에 대해서 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbr47Jt8eTKW"
   },
   "source": [
    "#### 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgtaUvV0eVoA",
    "outputId": "d186524f-8907-4556-c340-cd48f39f4711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sec Sleep....\n",
      "Sleep Fin 1sec\n",
      "2 sec Sleep....\n",
      "Sleep Fin 2sec\n",
      "3 sec Sleep....\n",
      "Sleep Fin 3sec\n",
      "4 sec Sleep....\n",
      "Sleep Fin 4sec\n",
      "Total Running Time :  10.02116322517395\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sleepFunction(t):\n",
    "  print(f\"{t} sec Sleep....\")\n",
    "  time.sleep(t)\n",
    "  print(f'Sleep Fin {t}sec')\n",
    "\n",
    "num_list = [1,2,3,4]\n",
    "\n",
    "\n",
    "for num in num_list:\n",
    "  sleepFunction(num)\n",
    "\n",
    "print('Total Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww6gqeys1m7b"
   },
   "source": [
    "위 예시에서 보시다시피 리스트를 단순 반복문을 돌리면 앞에 작업이 끝나기 전에는 \n",
    "뒷 작업을 진행  할 수 없어 총 대기시간의 합인 10초가 걸렸습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JR-8bt4ge0-e",
    "outputId": "8aee2c02-0948-40f2-dcf0-5086d797ac6b"
   },
   "outputs": [],
   "source": [
    "# # 멀티프로세싱\n",
    "# from multiprocessing import Pool\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# def sleepFunction(t):\n",
    "#   print(f\"{t} sec Sleep....\")\n",
    "#   time.sleep(t)\n",
    "#   print(f'Sleep Fin {t}sec')\n",
    "\n",
    "# num_list = [1,2,3,4]\n",
    "\n",
    "# pool = Pool(processes=4)\n",
    "# pool.map(sleepFunction, num_list)\n",
    "\n",
    "# print('Total Running Time : ', time.time() - start_time)\n",
    "\n",
    "# VSCode에서는 Windows 환경에서 if __name__ == \"__main__\": 이 없는 경우 \n",
    "# 멀티프로세싱에 문제가 생길 수 있습니다. 따라서 아래와 같이 수정해 주세요.\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sleepFunction(t):\n",
    "  print(f\"{t} sec Sleep....\")\n",
    "  time.sleep(t)\n",
    "  print(f'Sleep Fin {t}sec')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  num_list = [1,2,3,4]\n",
    "\n",
    "  pool = Pool(processes=4)\n",
    "  pool.map(sleepFunction, num_list)\n",
    "\n",
    "  print('Total Running Time : ', time.time() - start_time)\n",
    "\n",
    "# if __name__ == \"__main__\": 이 없는 경우는 모듈을 다른 곳에서 import할 때, \n",
    "# 해당 코드들이 불러와집니다. 이러한 경우 멀티프로세싱이 발생하면, \n",
    "# 다른 모듈에서도 해당 코드들이 중복으로 실행되어 버그가 발생할 수 있습니다. \n",
    "# 따라서 위 코드에서는 이러한 문제를 방지하기 위해 해당 코드를 \n",
    "# if __name__ == \"__main__\": 안에 작성하도록 수정했습니다.\n",
    "\n",
    "\n",
    "\n",
    "# 위 코드를 실행할 경우 무슨 이유에서인지?는 모르겠으나 윈도우 시스템에서는 runtimeError가 발생함. 따라서, \n",
    "# .py파일로 만들어서 실행하면 아래와 같은 결과값이 터미널에 나옵니다.\n",
    "    # (study) C:\\Users\\kjy15\\Desktop>C:/Users/kjy15/anaconda3/envs/study/python.exe c:/Users/kjy15/Desktop/crawling_news_team/AutoNewsDBworkBench/AutoNews_team_6/FileAutoSche_preWokrCode/preproc_summary/test_100.py\n",
    "    # 1 sec Sleep....\n",
    "    # 2 sec Sleep....\n",
    "    # 3 sec Sleep....\n",
    "    # 4 sec Sleep....\n",
    "    # Sleep Fin 1sec\n",
    "    # Sleep Fin 2sec\n",
    "    # Sleep Fin 3sec\n",
    "    # Sleep Fin 4sec\n",
    "    # Total Running Time :  4.184868097305298\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OUJ2PQv7f5KG"
   },
   "source": [
    "멀티프로세싱으로 4초의 시간만 걸린 것을 확인 할 수 있습니다. 앞서 스테이크 예시와 같이 작업할 수 있는 공간이 늘어났기 때문에 시간이 줄어든것입니다.\n",
    "</br></br>\n",
    "이 멀티프로세싱에는 pool과 process 두 가지 방법이 있다고 합니다.</br>pool은 작업 전체를 던져주고 알아서 처리해! 라는 방식이고 process는 직접 넌 이거 넌 저거 이런식으로 지정해주는 것이라고합니다. 아래는 위 작업을 process로 구현한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4eFine1k6kA",
    "outputId": "2cad4719-cc01-4e62-b336-43bb86aac655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Running Time :  0.1436164379119873\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as redfoxtistory\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sleepFunction(t):\n",
    "  sec = int(t)\n",
    "  print(f\"{sec} sec Sleep....\")\n",
    "  time.sleep(sec)\n",
    "  print(f'Sleep Fin {sec}sec')\n",
    "\n",
    "p1 = redfoxtistory.Process(target = sleepFunction, args = ('1'))\n",
    "p2 = redfoxtistory.Process(target = sleepFunction, args = ('2'))\n",
    "p3 = redfoxtistory.Process(target = sleepFunction, args = ('3'))\n",
    "p4 = redfoxtistory.Process(target = sleepFunction, args = ('4'))\n",
    "\n",
    "#작업대에서 해야할 일을 설정해주고 실행시켜줍니다.\n",
    "p1.start()\n",
    "p2.start()\n",
    "p3.start()\n",
    "p4.start()\n",
    "\n",
    "# join은 저희가 기존에 사용하던 str.join()과 완전 별개의 함수입니다.\n",
    "# multiprocessing안에 있는 함수로 이 함수는 내가 할당한 작업대의 작업이 끝날때까지 기다려라 라는 뜻입니다.\n",
    "# 아래 부분을 모두 활성화 해도 합이 아닌 가장 시간이 오래걸리는 작업대 시간만큼만 기다리는거보면 큰 작업장 개념이 따로 있나봅니다.\n",
    "# 좀 더 쉽게 이해하고 싶으시면 p3만 활성화 하고 실행해보시면 p4는 작업을 완료하지 못했는데 그냥 넘어가버립니다.\n",
    "# 다른 방법으로는 start와 조인을 번갈아 사용해보시면 됩니다. start -> join 하면 이 작업이 끝나야 다음 start가 실행됩니다.\n",
    "\n",
    "p1.join() # 1초 대기\n",
    "p2.join() # 2초 대기\n",
    "p3.join() # 3초 대기\n",
    "p4.join() # 4초 대기\n",
    "\n",
    "print('Total Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXLIItJ_l05j"
   },
   "source": [
    "#### 크롤링에 적용해보기\n",
    "\n",
    "우선 기본적인 뷰티풀숩으로 0201일의 경제-금융의 기사들을 크롤링해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YTrvArhG1IUP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake_useragent\n",
      "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: importlib-resources>=5.0 in c:\\users\\kjy15\\anaconda3\\envs\\study\\lib\\site-packages (from fake_useragent) (5.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\kjy15\\anaconda3\\envs\\study\\lib\\site-packages (from importlib-resources>=5.0->fake_useragent) (3.11.0)\n",
      "Installing collected packages: fake_useragent\n",
      "Successfully installed fake_useragent-1.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 저는 useragent를 페이크로 사용했습니다. 직접 입력해주시면 이부분은 실행 안하셔도 됩니다.\n",
    "%pip install fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0qKGYJjh0I3y"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "#useragent를 직접 입력하신다면 아래부분은 지우시고 headers부분만 직접 입력해주시면 됩니다.\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "fake_ua = ua.random\n",
    "\n",
    "headers = {\n",
    "    'user-agent' : fake_ua\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UrhLNw-Y0LQJ",
    "outputId": "7878ff27-91b2-485a-b52a-0f9b104c0a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 PAGE FIN URLS CNT :  20\n",
      "2 PAGE FIN URLS CNT :  20\n",
      "3 PAGE FIN URLS CNT :  20\n",
      "4 PAGE FIN URLS CNT :  20\n",
      "5 PAGE FIN URLS CNT :  20\n",
      "6 PAGE FIN URLS CNT :  20\n",
      "7 PAGE FIN URLS CNT :  20\n",
      "8 PAGE FIN URLS CNT :  20\n",
      "9 PAGE FIN URLS CNT :  20\n",
      "10 PAGE FIN URLS CNT :  20\n",
      "11 PAGE FIN URLS CNT :  20\n",
      "12 PAGE FIN URLS CNT :  20\n",
      "13 PAGE FIN URLS CNT :  20\n",
      "14 PAGE FIN URLS CNT :  20\n",
      "15 PAGE FIN URLS CNT :  20\n",
      "16 PAGE FIN URLS CNT :  20\n",
      "17 PAGE FIN URLS CNT :  20\n",
      "18 PAGE FIN URLS CNT :  20\n",
      "19 PAGE FIN URLS CNT :  20\n",
      "20 PAGE FIN URLS CNT :  20\n",
      "21 PAGE FIN URLS CNT :  20\n",
      "22 PAGE FIN URLS CNT :  20\n",
      "23 PAGE FIN URLS CNT :  20\n",
      "24 PAGE FIN URLS CNT :  20\n",
      "25 PAGE FIN URLS CNT :  20\n",
      "26 PAGE FIN URLS CNT :  20\n",
      "27 PAGE FIN URLS CNT :  18\n",
      "----------\n",
      "END PAGE :  27\n",
      "Total Urls :  538\n",
      "Check Running Time :  4.661794662475586\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "i=1\n",
    "total_urls = []\n",
    "urls = []\n",
    "while True:\n",
    "  test_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page={i}'\n",
    "  soup = bs(requests.get(test_url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "  now_url =[]\n",
    "  # url만 가져오기\n",
    "  for row in soup.select('#main_content > div.list_body.newsflash_body > ul > li'):\n",
    "    row = row.select_one('a')\n",
    "    now_url.append(row['href'])\n",
    "  \n",
    "  if urls ==  now_url:\n",
    "    break\n",
    "  \n",
    "  urls = now_url\n",
    "  total_urls += now_url\n",
    "\n",
    "  print(f\"{i} PAGE FIN URLS CNT : \", len(urls))\n",
    "  i += 1\n",
    "\n",
    "print(\"-\"*10)\n",
    "print('END PAGE : ', i-1)\n",
    "print('Total Urls : ', len(total_urls))\n",
    "print('Check Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQZlRMaS0VmW"
   },
   "source": [
    "#### 멀티프로세싱을 통한 크롤링\n",
    "\n",
    "이번에는 멀티프로세싱으로 크롤링 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkWwvvJBo2ZY",
    "outputId": "2dde8cc5-9e3d-4c62-8279-58d8040026ed"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "start_time = time.time()\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "\n",
    "# 페이지 탐색을 자동으로 종료하는 방법은 떠오르지 않아 마지막 페이지 정보를 먼저 가져오도록 했습니다.\n",
    "# 위 방법처럼 url들을 비교하는 방법이 있을 것 같긴한데 저도 좀 더 알아봐야할 것 같습니다. 혹시 발견하신 분은 공유해주시면 좋을 것 같아요!\n",
    "def getEndPage():\n",
    "  url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page=999'\n",
    "  soup = bs(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "  return soup.select_one('#main_content > div.paging > strong').text\n",
    "  \n",
    "\n",
    "def getUrls(page):\n",
    "  test_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page={page}'\n",
    "  \n",
    "  res = requests.get(test_url, headers=headers)\n",
    "\n",
    "  if res.status_code != 200:\n",
    "    print(i, \"page Request Error\")\n",
    "    return \n",
    "  \n",
    "  soup = bs(res.text, \"html.parser\")\n",
    "  now_urls =[]\n",
    "  \n",
    "  for row in soup.select('#main_content > div.list_body.newsflash_body > ul > li'):\n",
    "      row = row.select_one('a')\n",
    "      now_urls.append(row['href'])  \n",
    "  \n",
    "  print(f\"Crawing Fin {page} URLS CNT : \", len(now_urls))\n",
    "  return now_urls\n",
    "\n",
    "pages = [i for i in range(1, int(getEndPage())+1)]\n",
    "\n",
    "pool = Pool(processes=4)\n",
    "result = pool.map(getUrls, pages)\n",
    "\n",
    "urls = []\n",
    "for url in result:\n",
    "  urls += url\n",
    "\n",
    "\n",
    "print(\"-\"*10)\n",
    "print('URL CNT : ', len(urls))\n",
    "print('Check Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "start_time = time.time()\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "def getEndPage():\n",
    "    url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page=999'\n",
    "    soup = bs(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "    return soup.select_one('#main_content > div.paging > strong').text\n",
    "  \n",
    "\n",
    "def getUrls(page):\n",
    "    test_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page={page}'\n",
    "    res = requests.get(test_url, headers=headers)\n",
    "    \n",
    "    try:\n",
    "        if res.status_code != 200:\n",
    "            raise Exception(f\"{page} page Request Error\")\n",
    "        \n",
    "        soup = bs(res.text, \"html.parser\")\n",
    "        now_urls =[]\n",
    "        \n",
    "        for row in soup.select('#main_content > div.list_body.newsflash_body > ul > li'):\n",
    "            row = row.select_one('a')\n",
    "            now_urls.append(row['href'])  \n",
    "        \n",
    "        print(f\"Crawing Fin {page} URLS CNT : \", len(now_urls))\n",
    "        return now_urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "        \n",
    "    \n",
    "\n",
    "pages = [i for i in range(1, int(getEndPage())+1)]\n",
    "\n",
    "pool = Pool(processes=4)\n",
    "result = pool.map(getUrls, pages)\n",
    "\n",
    "urls = []\n",
    "for url in result:\n",
    "    urls += url\n",
    "\n",
    "print(\"-\"*10)\n",
    "print('URL CNT : ', len(urls))\n",
    "print('Check Running Time : ', time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku5rDMAyxyt5"
   },
   "source": [
    "핵심인 시간이 6초로 크게 줄어든게 눈에 보입니다. 그렇다면 작업대(processes)를 2배로 늘려보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpk5MVw4yK4z",
    "outputId": "0468d6f4-5b95-474f-b5c5-3dbe5d83d5b6"
   },
   "outputs": [],
   "source": [
    "# 위에서 작업했던 크롤링을 가져와서 그대로 사용해 보겠습니다.\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "start_time = time.time()\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "\n",
    "def getEndPage():\n",
    "  url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page=999'\n",
    "  soup = bs(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "  return soup.select_one('#main_content > div.paging > strong').text\n",
    "  \n",
    "\n",
    "def getUrls(page):\n",
    "  test_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page={page}'\n",
    "  \n",
    "  res = requests.get(test_url, headers=headers)\n",
    "\n",
    "  if res.status_code != 200:\n",
    "    print(i, \"page Request Error\")\n",
    "    return \n",
    "  \n",
    "  soup = bs(res.text, \"html.parser\")\n",
    "  now_urls =[]\n",
    "  \n",
    "  for row in soup.select('#main_content > div.list_body.newsflash_body > ul > li'):\n",
    "      row = row.select_one('a')\n",
    "      now_urls.append(row['href'])  \n",
    "  \n",
    "  print(f\"Crawing Fin {page} URLS CNT : \", len(now_urls))\n",
    "  return now_urls\n",
    "\n",
    "pages = [i for i in range(1, int(getEndPage())+1)]\n",
    "\n",
    "pool = Pool(processes=8)\n",
    "result = pool.map(getUrls, pages)\n",
    "\n",
    "urls = []\n",
    "for url in result:\n",
    "  urls += url\n",
    "\n",
    "\n",
    "print(\"-\"*10)\n",
    "print('URL CNT : ', len(urls))\n",
    "print('Check Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqiQNRWZyRfc"
   },
   "source": [
    "더 줄었네요! 그런데 여기서 중요한 점은 'processes'옵션의 숫자는 사용자의 컴퓨터의 성능에 따라 달라진다고 합니다.</br></br>\n",
    "컴퓨터 하드웨어 중 CPU를 평가할때 8코어 16코어 이렇게 이야기하는건 광고에서 강조하는 부분이라 한 번쯤은 들어보셨을거라 생각합니다. 이게 바로 프로세스를 처리할 수 있는 작업대 수를 결정하는 거라고 합니다. 따라서 작업대를 너무 많이 설정하면 오히려 속도 저하의 원인이 된다고 합니다.\n",
    "\n",
    "[멀티프로세스](https://superfastpython.com/multiprocessing-pool-num-workers/#What_is_a_CPU_and_What_is_a_CPU_Core)에 관련한 내용이 정리된 링크 공유로 마무리하겠습니다!</br>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
