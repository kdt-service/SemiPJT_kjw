크롤링 대상 사이트에 아이디 및 비밀번호를 입력해야 로그인할 수 있습니다.
BeautifulSoup를 사용하여 크롤링하는 방법을 알려주십시오.

BeautifulSoup는 HTML과 XML 파일을 파싱하는 파이썬 라이브러리이므로, 로그인이 필요한 사이트를 크롤링하기 위해서는 보통 requests 모듈과 BeautifulSoup 라이브러리를 함께 사용합니다. 로그인이 필요한 페이지에 접속하기 위해서는 먼저 로그인 페이지의 URL을 파악하고, 그 페이지에서 사용되는 아이디와 비밀번호 입력 폼의 이름을 알아내야 합니다.

다음은 예시로 http://example.com/login 이라는 로그인 페이지에 접속하여, 사용자명과 비밀번호를 입력하고 로그인을 시도하는 코드입니다.

python
Copy code
import requests
from bs4 import BeautifulSoup

# 로그인 정보
login_info = {
    'username': 'your_username',
    'password': 'your_password'
}

# 세션 생성
session = requests.Session()

# 로그인 페이지 가져오기
login_page = session.get('http://example.com/login')

# 로그인 폼 파싱하기
soup = BeautifulSoup(login_page.text, 'html.parser')
login_form = soup.find('form', {'id': 'login-form'})

# 로그인 폼에 포함된 input 요소 파싱하기
inputs = login_form.findAll('input')

# input 요소에서 로그인 정보에 맞게 입력할 데이터 추출하기
login_data = { i.get('name'): i.get('value') for i in inputs }

# 로그인 정보 추가하기
login_data.update(login_info)

# 로그인 시도
session.post('http://example.com/login', data=login_data)

# 로그인 후 페이지에 접근하여 크롤링하기
response = session.get('http://example.com/secret_page')

# 크롤링할 페이지의 HTML 파싱하기
soup = BeautifulSoup(response.text, 'html.parser')

# 원하는 정보 추출하기
# ...
위 코드에서는 requests 모듈을 사용하여 세션을 생성하고, session.get() 메서드를 사용하여 로그인 페이지에 접근합니다. 그리고 BeautifulSoup 라이브러리를 사용하여 해당 페이지의 HTML을 파싱한 후, 로그인 폼의 form 요소와 그 하위에 있는 input 요소들을 추출합니다.

그 다음, 로그인 정보를 담은 딕셔너리 객체를 생성한 후, 로그인 폼의 input 요소에서 추출한 정보와 합쳐서 로그인을 시도합니다. 로그인 성공 여부는 세션 객체를 사용하여 다른 페이지에 접근하여 확인할 수 있습니다. 만약 로그인에 실패한다면 session.post() 메서드에서 예외가 발생합니다.

마지막으로 성공적으로 로그인한 후, 다른 페이지에 접근하여 원하는 정보를 추출합니다. 이때, 로그인된 상태를 유지하기 위해서 requests.Session 객체를 사용하여 접근해야 합니다.