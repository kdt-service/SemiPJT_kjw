https://kind.krx.co.kr/corpgeneral/corpList.do?method=loadInitPage
종목과 종목코드를 각 페이지별로 크롤링하고, 그 크롤링 결과를
아래 사이트에  
https://finance.naver.com/item/coinfo.naver?code=000100
할당하여 PER, PBR, ROE, 배당수익율, 순이익율, 투자의견을 크롤링하려고 하면
어떻게 코딩을 해야 할까요? 필요한 라이브러리와 패키지는 무엇인가요? 

Scrapy 라이브러리만으로도 충분히 크롤링을 수행할 수 있습니다.

아래는 구체적인 코딩 예시입니다.


scrapy를 이용하여 상장된 모든 종목 코드와 종목명 수집하기

import scrapy


class MySpider(scrapy.Spider):
    name = 'my_spider'
    start_urls = ['https://kind.krx.co.kr/corpgeneral/corpList.do?method=loadInitPage']

    def parse(self, response):
        for tr in response.xpath('//*[@id="content"]/div[2]/div[1]/div/table/tbody/tr'):
            company_name = tr.xpath('./td[2]/a/text()').get()
            company_code = tr.xpath('./td[1]/a/text()').get()
            yield {'company_name': company_name, 'company_code': company_code}


네이버 증권에서 종목코드를 이용하여 데이터 수집하기

import scrapy

class MySpider(scrapy.Spider):
    name = 'my_spider'
    allowed_domains = ['finance.naver.com']
    start_urls = ['https://finance.naver.com/item/coinfo.naver?code=000100']

    def parse(self, response):
        company_code = response.url.split('=')[-1]
        per = response.xpath('//td[contains(text(),"PER")]/following-sibling::td[1]/text()').get()
        pbr = response.xpath('//td[contains(text(),"PBR")]/following-sibling::td[1]/text()').get()
        roe = response.xpath('//td[contains(text(),"ROE")]/following-sibling::td[1]/text()').get()
        dividend_yield = response.xpath('//td[contains(text(),"배당수익율")]/following-sibling::td[1]/text()').get()
        net_profit_margin = response.xpath('//td[contains(text(),"순이익율")]/following-sibling::td[1]/text()').get()
        investment_opinion = response.xpath('//span[contains(text(),"투자의견")]/following-sibling::span/text()').get()
        yield {'company_code': company_code, 'per': per, 'pbr': pbr, 'roe': roe, 
               'dividend_yield': dividend_yield, 'net_profit_margin': net_profit_margin, 
               'investment_opinion': investment_opinion}
위 예시에서는 Scrapy 라이브러리를 이용하여 웹페이지에서 데이터를 수집하고 처리할 수 있습니다. Scrapy는 크롤링을 보다 쉽게 만들어주는 다양한 기능들을 제공하므로, 웹페이지에서 데이터를 크롤링할 때 Scrapy를 이용하는 것이 효율적일 수 있습니다.

아래는 Selenium과 BeautifulSoup로 크롤링하기

1.
아래 사이트에서 종목명과 종목코드를 크롤링

전체
https://kind.krx.co.kr/corpgeneral/corpList.do?method=loadInitPage

유가증권
https://kind.krx.co.kr/corpgeneral/corpList.do?method=loadInitPage

코스닥
https://kind.krx.co.kr/corpgeneral/corpList.do?method=loadInitPage

사용해야할 라이브러리는 selenium라이브러리
이유는? 	위 사이트의 페이지 이동을 위한 링크가 JavaScript를 사용하여 
          	구현되어 있기 때문이다.
	Selenium 라이브러리를 사용하여 웹 브라우저를 제어하고, 
	find_element_by_css_selector() 함수를 사용하여 페이지 이동을 구현. 
	move_to_page() 함수는 이동할 페이지 번호를 받아 페이지 이동을 수행. 
	이후, BeautifulSoup 라이브러리를 사용하여 네이버증권사이트에서 
	데이터 수집을 진행하면 된다.
	또한, Selenium을 사용하여 웹 브라우저를 자동화하기 위해서는 반드시,
	해당 브라우저의 WebDriver가 필요. 이때, WebDriver의 버전이 
	사용하는 브라우저의 버전과 일치해야 한다. WebDriver를 설치하고 
	PATH 환경 변수에 등록하는 과정은 매우 귀찮지? 그러므로, 
	webdriver-manager패키지를 사용하여 자동으로 처리해야 한다.
	Selenium을 사용하여 ' 웹 브라우저를 자동화 '하기 위해서는 
 	Selenium 라이브러리와 함께 webdriver-manager 패키지를 설치하고 사용해야 한다. 반드시...
	웹페이지의 동적인 요소들을 제어하고, 브라우저별로 WebDriver를 일일이 설치하고 
	관리하는 불편함을 없애기 위해서다.

2. 
아래 사이트에서 입력변수와 종속변수를 크롤링
https://finance.naver.com/item/coinfo.naver?code=000100

사용해야할 라이브러리는 requests라이브러리와 BeautifulSoup라이브러리이다.
이유는? 	네이버증권 사이트는 JavaScript를 사용하지 않는 정적인 페이지이기 때문에 
	requests 라이브러리와 BeautifulSoup 라이브러리를 사용하여 크롤링이 가능.
	지난번 2조 3조처럼 반드시 해당 사이트에서 데이터를 크롤링할 때는 
	robots을 확인하는 코딩을 반드시 넣어야 한다.

3. 
최재진선생님 가이드를 반영해야 함.
가이드 : 종목을 선정하는 로직을 수치를 조건으로 해서 '하드로직'으로 하면 안된다. 왜냐하면?
	랜덤포레스트에 해당 로직을 그대로 학습할 가능성이 있기 때문이다.
	따라서, 선생님께서는 종목을 선정하는 방법을 수치를 조건으로 주지 말고,
	다른 방법을 사용하라고 조언해 주셨슴. 예를 들어, 유튜버 경제전문가, 분석전문가가 
	추천하는 주식 종목, 혹은 각종 리서치에서 제공하는 레포트에서 주식 종목명만 
	개체명을 인식해서 주식 종목 선정 비선정 여부를 판단하는 방법임
결론 : 유튜버 경제전문가, 분석전문가 추천하는 주식 종목 또는 리서치에서 제공하는 
	레포트에서 주식 종목명만 개체명을 인식하는 것도 좋은 방법이지만, 각 종목별
	연결이 이미 되어 있어서 크롤링이 편리하다고 판단되는 아래의 방법을 선택.
	네이버 증권사이트에서 각 종목별 '투자의견 컨센서스'의 '투자의견' 점수를 긁어오는
	방법임.