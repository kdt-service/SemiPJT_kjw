{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ddb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# 크롤링할 기간 설정\n",
    "start_date = datetime(2023, 2, 1)\n",
    "end_date = datetime(2023, 2, 2)\n",
    "\n",
    "# 크롤링할 URL 설정\n",
    "url = \"https://news.daum.net/breakingnews/digital\"\n",
    "\n",
    "# 크롤링할 기사 링크를 저장할 리스트 초기화\n",
    "links = set()\n",
    "\n",
    "# 주어진 기간 동안 각 페이지의 기사 링크 추출\n",
    "while start_date <= end_date:\n",
    "    \n",
    "    # 현재 날짜에 해당하는 페이지 URL 생성\n",
    "    date_str = start_date.strftime(\"%Y%m%d\")\n",
    "    print(f\"\\n\\n☞☞☞☞☞날짜:  {date_str}\")\n",
    "    \n",
    "    page_url = f\"{url}?regDate={date_str}\"\n",
    "    print(f\"☞☞☞☞☞Page_URL:   {page_url}\\n\\n\")\n",
    "    \n",
    "    num_page = 1\n",
    "    \n",
    "    last_page = False\n",
    "\n",
    "    # 각 페이지의 기사 링크 추출\n",
    "    while True:\n",
    "        page_url_num = f\"{page_url}&page={num_page}\"\n",
    "        \n",
    "        \n",
    "        # User-Agent 정보 추가\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        res = requests.get(page_url_num, headers=headers)\n",
    "\n",
    "        # HTTP 요청이 성공했는지 확인\n",
    "        if res.status_code != 200:\n",
    "            print(f\"Failed to retrieve page: {page_url_num}\")\n",
    "            continue\n",
    "\n",
    "        # BeautifulSoup으로 HTML 파싱\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "        # robots.txt 파일 참조\n",
    "        robot_check = soup.find('meta', attrs={'name': 'robots'})\n",
    "        if robot_check and 'nofollow' in robot_check['content']:\n",
    "            print('Robots.txt 를 준수하지 않습니다.')\n",
    "            break\n",
    "\n",
    "        # 각 기사의 링크를 추출하여 리스트에 추가\n",
    "        article_links = soup.select(\".list_news2 a.link_txt\")\n",
    "        #print(f\"\\n☞Article_Links:   {article_links}\")\n",
    "        \n",
    "        if not article_links:\n",
    "            print(f\"\\n☞ CSS선택자 .list_news2 a.link_txt 에 해당하는 링크를 찾을 수 없습니다.\")\n",
    "            break\n",
    "\n",
    "        for link in article_links:\n",
    "            article_url = link[\"href\"]\n",
    "            article_res = requests.get(article_url)\n",
    "            \n",
    "            # HTTP 요청이 성공했는지? 여부를 확인하기 위한 코드\n",
    "            if article_res.status_code !=200:\n",
    "                print(f\"HTTP요청이 실패했슴:  {article_url}\")\n",
    "                continue\n",
    "            \n",
    "            article_soup = BeautifulSoup(article_res.content, \"html.parser\")\n",
    "            date = article_soup.select_one(\".info_view .txt_info\")\n",
    "            \n",
    "            # 작성일자 추출\n",
    "            \n",
    "            # date변수가 None 값을 가질 경우 건너뛰도록 아래와 같이 코딩하거나,\n",
    "            if date:\n",
    "                date = re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\s\\d{2}:\\d{2}\", date.get_text())\n",
    "                if date:\n",
    "                    date = date.group()\n",
    "                    date = datetime.strptime(date, \"%Y.%m.%d %H:%M\")\n",
    "                    print(f\"date:  {date}\")\n",
    "             # 정규표현식을 수정하여 일치하는 항목이 없는 경우 None 대신 빈 문자열을 반환하도록  아래와 같이 코딩\n",
    "#             date = re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\s\\d{2}:\\d{2}\", date.get_text() or \"\")\n",
    "#             if date:\n",
    "#                 date = date.group()\n",
    "#                 date = datetime.strptime(date, \"%Y.%m.%d %H:%M\")\n",
    "#                 print(f\"date:  {date}\")\n",
    "                \n",
    "\n",
    "            if (article_url, date):                     \n",
    "                links.add((article_url, date))\n",
    "                                \n",
    "\n",
    "        # 추출된 링크를 이용하여 기사 내용 크롤링        \n",
    "        for link, date in links:\n",
    "        \n",
    "            # 기사 URL 출력 (테스트용)\n",
    "            print(f\"\\n\\n☞기사링크: {link}\")\n",
    "\n",
    "            # 기사 HTML 코드 가져오기\n",
    "            res = requests.get(link)\n",
    "\n",
    "            # HTTP 요청이 성공했는지 확인\n",
    "            if res.status_code != 200:\n",
    "                print(f\"Failed to retrieve article: {link}\")\n",
    "                continue\n",
    "\n",
    "            # BeautifulSoup으로 HTML 파싱\n",
    "            soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "            # 기사 제목 추출\n",
    "            title = soup.select_one(\".tit_view\").get_text()\n",
    "\n",
    "            # 기사 본문 추출\n",
    "            contents = soup.select(\".article_view p\")\n",
    "            content = \"\\n\".join([c.get_text().strip() for c in contents])\n",
    "\n",
    "            # 추출한 제목과 내용 출력\n",
    "            print(f\"☞기사제목: {title}\")\n",
    "            print(f\"☞작성일자: {date_str}\")\n",
    "            print(f\"☞Page_URL_NUM :   {page_url_num}\")\n",
    "            print(f\"☞기사내용: \\n{content}\")\n",
    "#            print(f\"☞페이지: {num_page}\") #page_url_num의 num_page를 확인하고 싶을때 출력\n",
    "        \n",
    "        # 마지막 페이지인지? 여부를 검사하여 다음 페이지로 이동\n",
    "        \n",
    "        last_page = (len(article_links) <= 14)\n",
    "            # 한페이지에 15개의 기사가 있으므로 기사의 개수가 14개이하일때 마지막 페이지로 간주하도록 설정\n",
    "            # IT/경제/문화 모두 한페이지에 15개의 기사가 있다.\n",
    "        \n",
    "        if last_page:\n",
    "            print(f\"\\n☞ {date}의 마지막 페이지입니다. {num_page}\")\n",
    "            break\n",
    "        num_page += 1\n",
    "\n",
    "    # 다음 날짜로 이동\n",
    "    start_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b44ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
